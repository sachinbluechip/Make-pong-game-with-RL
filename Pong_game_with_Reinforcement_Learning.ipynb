{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sachinbluechip/Make-pong-game-with-RL/blob/main/Pong_game_with_Reinforcement_Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "EvdePP-VyVWp",
        "trusted": true
      },
      "cell_type": "code",
      "source": [
        "!apt-get install -y xvfb python-opengl x11-utils\n",
        "!apt-get install -y --no-install-recommends ffmpeg\n",
        "!pip install ffmpeg\n",
        "!pip install gym pyvirtualdisplay scikit-video #> /dev/null 2>&1\n",
        "#!pip install 'gym[box2d]'\n",
        "!pip install atari_py\n",
        "\n",
        "#%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "\n",
        "import numpy as np\n",
        "import base64, io, time, gym\n",
        "import IPython, functools\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "\n",
        "!pip install mitdeeplearning\n",
        "import mitdeeplearning as mdl"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Eu6Mqxc720ST"
      },
      "cell_type": "markdown",
      "source": [
        "# Pong\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "srZ4YE29isuA"
      },
      "cell_type": "markdown",
      "source": [
        "Define and inspect the Pong environment\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "lbYHLr66i15n",
        "trusted": true
      },
      "cell_type": "code",
      "source": [
        "env = gym.make(\"Pong-v0\", frameskip=5)\n",
        "env.seed(1); # for reproducibility"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0yX4GWvxjnHS",
        "trusted": true
      },
      "cell_type": "code",
      "source": [
        "print(\"Environment has observation space =\", env.observation_space)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Iuy9oPc1kag3",
        "trusted": true
      },
      "cell_type": "code",
      "source": [
        "n_actions = env.action_space.n\n",
        "print(\"Number of possible actions that the agent can choose from =\", n_actions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9-fghDRigUE5"
      },
      "cell_type": "markdown",
      "source": [
        "3.7 Define the Pong agent\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "IJiqbFYpgYRH",
        "trusted": true
      },
      "cell_type": "code",
      "source": [
        "### Define the Pong agent ###\n",
        "\n",
        "# Functionally define layers for convenience\n",
        "# All convolutional layers will have ReLu activation\n",
        "Conv2D = functools.partial(tf.keras.layers.Conv2D, padding='same', activation='relu')\n",
        "Flatten = tf.keras.layers.Flatten\n",
        "Dense = tf.keras.layers.Dense\n",
        "\n",
        "# Defines a CNN for the Pong agent\n",
        "def create_pong_model():\n",
        "  model = tf.keras.models.Sequential([\n",
        "    # Convolutional layers\n",
        "    # First, 16 7x7 filters with 4x4 stride\n",
        "    Conv2D(filters=16, kernel_size=7, strides=4),\n",
        "\n",
        "    # TODO: define convolutional layers with 32 5x5 filters and 2x2 stride\n",
        "    Conv2D(filters=32, kernel_size=5, strides=2),\n",
        "\n",
        "    # TODO: define convolutional layers with 48 3x3 filters and 2x2 stride\n",
        "    Conv2D(filters=48, kernel_size=3, strides=2),\n",
        "\n",
        "    Flatten(),\n",
        "\n",
        "    # Fully connected layer and output\n",
        "    Dense(units=64, activation='relu'),\n",
        "    # TODO: define the output dimension of the last Dense layer.\n",
        "    # Pay attention to the space the agent needs to act in\n",
        "    Dense(units=n_actions, activation=None)\n",
        "\n",
        "  ])\n",
        "  return model\n",
        "\n",
        "pong_model = create_pong_model()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "l0RvqOVkmc2r"
      },
      "cell_type": "markdown",
      "source": [
        "Pong-specific functions\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "iEZG2o50luLu",
        "trusted": true
      },
      "cell_type": "code",
      "source": [
        "### Pong reward function ###\n",
        "\n",
        "# Compute normalized, discounted rewards for Pong (i.e., return)\n",
        "# Arguments:\n",
        "#   rewards: reward at timesteps in episode\n",
        "#   gamma: discounting factor. Note increase to 0.99 -- rate of depreciation will be slower.\n",
        "# Returns:\n",
        "#   normalized discounted reward\n",
        "def discount_rewards(rewards, gamma=0.99):\n",
        "  discounted_rewards = np.zeros_like(rewards)\n",
        "  R = 0\n",
        "  for t in reversed(range(0, len(rewards))):\n",
        "      # NEW: Reset the sum if the reward is not 0 (the game has ended!)\n",
        "      if rewards[t] != 0:\n",
        "        R = 0\n",
        "      # update the total discounted reward as before\n",
        "      R = R * gamma + rewards[t]\n",
        "      discounted_rewards[t] = R\n",
        "\n",
        "  return normalize(discounted_rewards)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "no5IIYtFm8pI",
        "trusted": true
      },
      "cell_type": "code",
      "source": [
        "observation = env.reset()\n",
        "for i in range(30):\n",
        "  observation, _,_,_ = env.step(0)\n",
        "observation_pp = mdl.lab3.preprocess_pong(observation)\n",
        "\n",
        "f = plt.figure(figsize=(10,3))\n",
        "ax = f.add_subplot(121)\n",
        "ax2 = f.add_subplot(122)\n",
        "ax.imshow(observation); ax.grid(False);\n",
        "ax2.imshow(np.squeeze(observation_pp)); ax2.grid(False); plt.title('Preprocessed Observation');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mRqcaDQ1pm3x"
      },
      "cell_type": "markdown",
      "source": [
        "Training Pong\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "xCwyQQrPnkZG",
        "trusted": true
      },
      "cell_type": "code",
      "source": [
        "### Training Pong ###\n",
        "\n",
        "# Hyperparameters\n",
        "learning_rate=1e-4\n",
        "MAX_ITERS = 500 # increase the maximum number of episodes, since Pong is more complex!\n",
        "\n",
        "# Model and optimizer\n",
        "pong_model = create_pong_model()\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
        "\n",
        "# plotting\n",
        "smoothed_reward = mdl.util.LossHistory(smoothing_factor=0.9)\n",
        "plotter = mdl.util.PeriodicPlotter(sec=5, xlabel='Iterations', ylabel='Rewards')\n",
        "memory = Memory()\n",
        "\n",
        "for i_episode in range(MAX_ITERS):\n",
        "\n",
        "  plotter.plot(smoothed_reward.get())\n",
        "\n",
        "  # Restart the environment\n",
        "  observation = env.reset()\n",
        "  previous_frame = mdl.lab3.preprocess_pong(observation)\n",
        "\n",
        "  while True:\n",
        "      # Pre-process image\n",
        "      current_frame = mdl.lab3.preprocess_pong(observation)\n",
        "\n",
        "      '''TODO: determine the observation change\n",
        "      Hint: this is the difference between the past two frames'''\n",
        "      obs_change = current_frame - previous_frame\n",
        "\n",
        "      '''TODO: choose an action for the pong model, using the frame difference, and evaluate'''\n",
        "      action = choose_action(pong_model, obs_change)\n",
        "      # Take the chosen action\n",
        "      next_observation, reward, done, info = env.step(action)\n",
        "\n",
        "      '''TODO: save the observed frame difference, the action that was taken, and the resulting reward!'''\n",
        "      memory.add_to_memory(obs_change, action, reward)\n",
        "\n",
        "      # is the episode over? did you crash or do so well that you're done?\n",
        "      if done:\n",
        "          # determine total reward and keep a record of this\n",
        "          total_reward = sum(memory.rewards)\n",
        "          smoothed_reward.append( total_reward )\n",
        "\n",
        "          # begin training\n",
        "          train_step(pong_model,\n",
        "                     optimizer,\n",
        "                     observations = np.stack(memory.observations, 0),\n",
        "                     actions = np.array(memory.actions),\n",
        "                     discounted_rewards = discount_rewards(memory.rewards))\n",
        "\n",
        "          memory.clear()\n",
        "          break\n",
        "\n",
        "      observation = next_observation\n",
        "      previous_frame = current_frame"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TvHXbkL0tR6M",
        "trusted": true
      },
      "cell_type": "code",
      "source": [
        "saved_pong = mdl.lab3.save_video_of_model(\n",
        "    pong_model, \"Pong-v0\", obs_diff=True,\n",
        "    pp_fn=mdl.lab3.preprocess_pong)\n",
        "mdl.lab3.play_video(saved_pong)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "jrI6q7RmWQam"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}